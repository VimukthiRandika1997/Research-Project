{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6cc11b-bcde-4190-833b-478becf690df",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "abd9a52e-cba6-43fa-9514-a40406048ffe",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "# Pytorch Geometric Workflow...\n",
    "\n",
    "- `__check_input__(**kwargs)`: check **SparseTensor** or not\n",
    "- `__collect__(**kwargs)`: Contruct the message of **node i**, for every node in the graph\n",
    "- `message(**kwargs)`: construct the **node i**'s message\n",
    "- `aggregate(**kwargs)`: message aggregation: max, min, mean, add\n",
    "- `update(**kwargs)`: update the features with aggregated message + current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576941c0-bb20-4d74-a0e3-7f50967b0484",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "1b635303-29ed-4998-a5bb-583c7c3f3a2b",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-06T04:03:22.037044336Z",
     "start_time": "2023-07-06T04:03:18.314138971Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch.nn.init import xavier_uniform_, zeros_\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00640a7-1457-411c-a5e8-bae324918c2b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "0bda0e28-b479-4c2b-9e2d-576468c8437e",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-06T04:03:22.080963991Z",
     "start_time": "2023-07-06T04:03:22.076161190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features\n",
      " tensor([[0.5345, 0.0603, 0.5653, 0.4170, 0.9076],\n",
      "        [0.2998, 0.2692, 0.2079, 0.0464, 0.2558],\n",
      "        [0.3123, 0.9619, 0.8403, 0.2705, 0.5712],\n",
      "        [0.2150, 0.3433, 0.1995, 0.6572, 0.4067]]) torch.Size([4, 5])\n",
      "\n",
      "edge_index\n",
      " tensor([[0, 0, 0, 1, 1, 2, 3, 3],\n",
      "        [1, 2, 3, 3, 0, 0, 0, 2]]) torch.Size([2, 8])\n",
      "\n",
      "Edge features\n",
      " tensor([[0.1068, 0.6574, 0.3379, 0.0812, 0.0539],\n",
      "        [0.5815, 0.1062, 0.6312, 0.4412, 0.5107],\n",
      "        [0.6821, 0.3295, 0.1306, 0.7818, 0.6742],\n",
      "        [0.1610, 0.1011, 0.7122, 0.3853, 0.9222],\n",
      "        [0.9187, 0.0547, 0.3643, 0.3242, 0.5840],\n",
      "        [0.9987, 0.3162, 0.3334, 0.5726, 0.5883],\n",
      "        [0.6838, 0.8965, 0.4611, 0.0078, 0.7463],\n",
      "        [0.8724, 0.6355, 0.2915, 0.5650, 0.4002]]) torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 4 # number of nodes in the graph\n",
    "embed_size = 5 # initial node features\n",
    "\n",
    "# Creating node features\n",
    "node_feat = torch.rand((num_nodes, embed_size), dtype=torch.float)\n",
    "x = node_feat\n",
    "print('Node features\\n', node_feat, node_feat.shape)\n",
    "\n",
    "# Creating COO format edge_indexes for the graph\n",
    "src_index = torch.tensor([0,0,0,1,1,2,3,3], dtype=torch.long)\n",
    "target_index = torch.tensor([1,2,3,3,0,0,0,2], dtype=torch.long)\n",
    "edge_index = torch.zeros(size=(2, src_index.size()[0]), dtype=torch.int64)\n",
    "edge_index[0] = src_index\n",
    "edge_index[1] = target_index\n",
    "print('\\nedge_index\\n', edge_index, edge_index.shape)\n",
    "\n",
    "# Creating edge features\n",
    "x_edge = torch.rand((edge_index.shape[1], 5), dtype=torch.float) # (num_edges, embed_size)\n",
    "print('\\nEdge features\\n', x_edge, x_edge.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df567d0d-d8ea-44d6-91bd-080819cec67c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "5fe7a8d9-0991-475e-b10a-3bff3588b843",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623a4f2f-811f-4f2e-91fd-5030cc52975c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "36693d76-218a-4761-94b0-0707bda598eb",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-06T04:03:22.161571843Z",
     "start_time": "2023-07-06T04:03:22.077326341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100]) torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "gat = GATv2Conv(5, 100)\n",
    "res1, res2 = gat.forward(x, edge_index, return_attention_weights=True)\n",
    "print(res1[0].shape, res2[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279880b8-60f1-4911-a1d5-351739917288",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "561c961b-1b6c-4128-992d-8a053c63ef31",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-06T04:03:22.196704276Z",
     "start_time": "2023-07-06T04:03:22.164601477Z"
    }
   },
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, \n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        concat=True,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0.0,\n",
    "        add_self_loops=True,\n",
    "        bias=True,\n",
    "        share_weights=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add' , **kwargs) # defines the aggregation method: `aggr='add'`\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.share_weights = share_weights\n",
    "        \n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias,)\n",
    "        \n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "            \n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        \n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self._alpha = None\n",
    "\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)        \n",
    "        \n",
    "    def forward(self, x, edge_index, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "        \n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        x_l = None # for source nodes\n",
    "        x_r = None # for target nodes\n",
    "        \n",
    "        x_l = self.lin_l(x).view(-1, H, C) # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "            \n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "        \n",
    "        if self.add_self_loops: # Adding self-loops for the graph...\n",
    "            if torch.is_tensor(edge_index):\n",
    "                num_nodes = x_l.size(0)\n",
    "                if x_r is not None:\n",
    "                    num_nodes = min(num_nodes, x_r.size(0))\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "                \n",
    "        # Start propagating info...: contruct message -> aggreate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), size=None) # (N, NH, H_out)\n",
    "        \n",
    "        alpha = self._alpha # (#edges, NH)\n",
    "        assert alpha is not None\n",
    "        self._alpha = None\n",
    "        \n",
    "        if self.concat: # (N, NH, H_out) -> (N, NH * H_out)\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1) # (N, NH, H_out) -> (N, H_out)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        \n",
    "            \n",
    "        # Returning attetion weights with computed hidden features    \n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if torch.is_tensor(edge_index):\n",
    "                return out, (edge_index, alpha) # Depends on 'concat', ((2, #edges), (#edges, NH))\n",
    "        else:   \n",
    "            return out # Depends on 'concat': (N, NH * H_out) || (N, H_out)\n",
    "    \n",
    "    def message(self, x_j, x_i, index, size_i):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "        \n",
    "        x = x_i + x_j # adding(element-wise) source and target node features together to calculate atttention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i) # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha # (#edges, NH)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training) # randomly dropping attention during training\n",
    "        \n",
    "        return x_j * alpha.unsqueeze(-1) # (#edges, NH, H_out)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d197fb8-a7c1-4cee-9e4a-14a5d667a794",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "b656e0db-2ff0-4741-a0df-1950ba7ee8c0",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-06T04:03:22.253881909Z",
     "start_time": "2023-07-06T04:03:22.187116412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.0209,  0.0724, -0.1171, -0.0533,  0.0359,  0.0240, -0.0938, -0.0585,\n          -0.1673, -0.0335],\n         [ 0.0226,  0.0430, -0.1237, -0.0295,  0.0446,  0.0336, -0.0909, -0.0249,\n          -0.1550, -0.0169],\n         [ 0.0206,  0.0907, -0.1357, -0.0814,  0.0266,  0.0107, -0.1131, -0.0627,\n          -0.1599, -0.0364],\n         [ 0.0341,  0.0587, -0.0838, -0.0190,  0.0475,  0.0290, -0.0979, -0.0074,\n          -0.1709, -0.0145]], grad_fn=<AddBackward0>),\n (tensor([[0, 0, 0, 1, 1, 2, 3, 3, 0, 1, 2, 3],\n          [1, 2, 3, 3, 0, 0, 0, 2, 0, 1, 2, 3]]),\n  tensor([[0.4209, 0.4899, 0.4841, 0.5050, 0.5091, 0.4941, 0.5088, 0.5234],\n          [0.3030, 0.3273, 0.3238, 0.3514, 0.3509, 0.3355, 0.3368, 0.3343],\n          [0.2909, 0.3182, 0.3288, 0.3447, 0.3433, 0.3317, 0.3300, 0.3444],\n          [0.3809, 0.3313, 0.3401, 0.3379, 0.3270, 0.3488, 0.3223, 0.3196],\n          [0.2857, 0.2542, 0.2541, 0.2562, 0.2487, 0.2557, 0.2443, 0.2304],\n          [0.2351, 0.2329, 0.2431, 0.2419, 0.2374, 0.2717, 0.2458, 0.2584],\n          [0.2588, 0.2688, 0.2523, 0.2405, 0.2559, 0.2229, 0.2591, 0.2520],\n          [0.3580, 0.3604, 0.3325, 0.3233, 0.3371, 0.2994, 0.3356, 0.3280],\n          [0.2204, 0.2441, 0.2505, 0.2614, 0.2579, 0.2497, 0.2508, 0.2592],\n          [0.5791, 0.5101, 0.5159, 0.4950, 0.4909, 0.5059, 0.4912, 0.4766],\n          [0.3390, 0.3122, 0.3438, 0.3253, 0.3121, 0.3651, 0.3276, 0.3377],\n          [0.3282, 0.3504, 0.3312, 0.3174, 0.3298, 0.3195, 0.3476, 0.3360]],\n         grad_fn=<DifferentiableGraphBackward>)))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat = GAT(embed_size, 10, heads=8, concat=False, share_weights=True)\n",
    "gat.forward(x, edge_index, return_attention_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ca2ad-135e-4276-bf9c-0746e62247dc",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "38fc3fda-2866-44f6-b895-8385e84b9de7",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9ebc33-bdcf-4b45-9da7-153c67143632",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "df502a79-e59d-44b9-a896-45a32e6c3813",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-06T04:03:22.254435451Z",
     "start_time": "2023-07-06T04:03:22.252684168Z"
    }
   },
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, \n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        concat=True,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0.0,\n",
    "        add_self_loops=True,\n",
    "        bias=True,\n",
    "        share_weights=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add' , **kwargs) # defines the aggregation method: `aggr='add'`\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.share_weights = share_weights\n",
    "        \n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias,)\n",
    "        \n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "            \n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels * 2))\n",
    "        \n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self._alpha = None\n",
    "\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)        \n",
    "        \n",
    "    def forward(self, x, edge_index, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "        \n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        x_src = None # for source nodes\n",
    "        x_dst = None # for target nodes\n",
    "        \n",
    "        x_src = self.lin_l(x).view(-1, H, C) # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_dst = x_src\n",
    "        else:\n",
    "            x_dst = self.lin_r(x).view(-1, H, C)\n",
    "            \n",
    "        assert x_src is not None\n",
    "        assert x_dst is not None\n",
    "        \n",
    "        x = (x_src, x_dst)\n",
    "        a_src = x_src\n",
    "        a_dst = x_dst\n",
    "        a = (a_src, a_dst)\n",
    "        \n",
    "        if self.add_self_loops: # Adding self-loops for the graph...\n",
    "            if torch.is_tensor(edge_index):\n",
    "                num_nodes = x_src.size(0)\n",
    "                if x_dst is not None:\n",
    "                    num_nodes = min(num_nodes, x_dst.size(0))\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "                \n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=x, a=a, size=None) # (N, NH, H_out)\n",
    "        \n",
    "        alpha = self._alpha # (#edges, NH)\n",
    "        assert alpha is not None\n",
    "        self._alpha = None\n",
    "        \n",
    "        if self.concat: # (N, NH, H_out) -> (N, NH * H_out)\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1) # (N, NH, H_out) -> (N, H_out)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        \n",
    "            \n",
    "        # Returning attention weights with computed hidden features\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if torch.is_tensor(edge_index):\n",
    "                return out, (edge_index, alpha) # Depends on 'concat', ((2, #edges), (#edges, NH))\n",
    "        else:   \n",
    "            return out # Depends on 'concat': (N, NH * H_out) || (N, H_out)\n",
    "    \n",
    "    def message(self, x_j, x_i, a_i, a_j, index, size_i):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "        # print(a_i.shape, a_j.shape)\n",
    "        \n",
    "        # x = x_i + x_j # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = torch.cat([x_i, x_j], dim=-1)\n",
    "        # print(x.shape)\n",
    "        # print(self.att.shape)\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1)\n",
    "        # print(alpha.shape)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i) # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha # (#edges, NH)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training) # randomly dropping attention during training\n",
    "        \n",
    "        return x_j * alpha.unsqueeze(-1) # (#edges, NH, H_out)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda3b04f-b370-4e53-87fa-ed18b1f0c378",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "329bd66f-10dc-47bf-83f8-f28c551dd9b8",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-06T04:03:22.342716698Z",
     "start_time": "2023-07-06T04:03:22.253153932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "torch.Size([12, 8])\n",
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "gat = GAT(embed_size, 10, heads=8, concat=False, share_weights=True)\n",
    "res = gat.forward(x, edge_index, return_attention_weights=True)\n",
    "out, edge_index_returned_with_attention_weights = res[0], res[1]\n",
    "edge_index_returned, attention_weights = edge_index_returned_with_attention_weights\n",
    "print(out.shape)\n",
    "print(attention_weights.shape)\n",
    "print(edge_index_returned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# My Arch 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class EIN(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        concat=True,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0.0,\n",
    "        add_self_loops=True,\n",
    "        edge_dim=None,\n",
    "        fill_value='mean',\n",
    "        bias=True,\n",
    "        share_weights=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add' , **kwargs) # defines the aggregation method: `aggr='add'`\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.share_weights = share_weights\n",
    "        self.edge_dim = edge_dim\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        # For influence mechanism\n",
    "        # self.inf = Linear(edge_dim, out_channels)\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l = None # for source nodes\n",
    "        x_r = None # for target nodes\n",
    "\n",
    "        x_l = self.lin_l(x).view(-1, H, C) # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        if self.add_self_loops: # Adding self-loops for the graph...\n",
    "            if torch.is_tensor(edge_index):\n",
    "                if edge_attr is not None: # edge_attr is available -> add edge_attr for newly created self-loops: default `mean`\n",
    "                    num_nodes = x_l.size(0)\n",
    "                    if x_r is not None:\n",
    "                        num_nodes = min(num_nodes, x_r.size(0))\n",
    "                    edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "                    edge_index, edge_attr = add_self_loops(edge_index, edge_attr, fill_value=self.fill_value, num_nodes=num_nodes)\n",
    "                else:\n",
    "                    num_nodes = x_l.size(0)\n",
    "                    if x_r is not None:\n",
    "                        num_nodes = min(num_nodes, x_r.size(0))\n",
    "                    edge_index, _ = remove_self_loops(edge_index)\n",
    "                    edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "\n",
    "        # Check the edge features shape: test_case\n",
    "        if edge_attr is not None:\n",
    "            print(f'edge_features shape: {edge_attr.shape}')\n",
    "        else:\n",
    "            print('No edge features!')\n",
    "\n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr, size=None) # (N, NH, H_out)\n",
    "\n",
    "        alpha = self._alpha # (#edges, NH)\n",
    "        assert alpha is not None\n",
    "        self._alpha = None\n",
    "\n",
    "        if self.concat: # (N, NH, H_out) -> (N, NH * H_out)\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1) # (N, NH, H_out) -> (N, H_out)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        # Influence mechanism\n",
    "        res = alpha.mean(dim=1, keepdims=True) * edge_attr\n",
    "        print('inf: ', res.shape)\n",
    "\n",
    "        # Returning attention weights with computed hidden features\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if torch.is_tensor(edge_index):\n",
    "                if self.concat:\n",
    "                    return out, (edge_index, alpha) # Depends on 'concat', ((2, #edges), (#edges, NH))\n",
    "                return out, (edge_index, alpha.mean(dim=1, keepdims=True))\n",
    "        else:\n",
    "            return out # Depends on 'concat': (N, NH * H_out) || (N, H_out)\n",
    "\n",
    "    def message(self, x_j, x_i, index, size_i):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "\n",
    "        x = x_i + x_j # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1) # (#edges, NH)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i) # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha # (#edges, NH)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training) # randomly dropping attention during training\n",
    "\n",
    "        return x_j * alpha.unsqueeze(-1) # (#edges, NH, H_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T04:39:22.494555878Z",
     "start_time": "2023-07-06T04:39:22.471722092Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_features shape: torch.Size([12, 5])\n",
      "inf:  torch.Size([12, 5])\n",
      "torch.Size([4, 10])\n",
      "tensor([[0.5031],\n",
      "        [0.3298],\n",
      "        [0.3339],\n",
      "        [0.3296],\n",
      "        [0.2443],\n",
      "        [0.2584],\n",
      "        [0.2505],\n",
      "        [0.3327],\n",
      "        [0.2469],\n",
      "        [0.4969],\n",
      "        [0.3375],\n",
      "        [0.3365]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0, 0, 0, 1, 1, 2, 3, 3, 0, 1, 2, 3],\n",
      "        [1, 2, 3, 3, 0, 0, 0, 2, 0, 1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "ein = EIN(embed_size, 10, heads=8, concat=False,)\n",
    "res = ein.forward(x, edge_index, edge_attr=x_edge,return_attention_weights=True)\n",
    "out, r = res\n",
    "# print(r[1].shape)\n",
    "print(out.shape)\n",
    "print(r[1])\n",
    "print(r[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T04:39:23.336483054Z",
     "start_time": "2023-07-06T04:39:23.286361477Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 5])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_edge.shape\n",
    "r[1][:8].shape\n",
    "# r[1][:8].reshape(8, 1) * x_edge\n",
    "temp = r[1][:8] * x_edge\n",
    "temp.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T04:39:33.003311968Z",
     "start_time": "2023-07-06T04:39:32.997347310Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "from torch_scatter import gather_csr, scatter, segment_csr\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:18:14.195837254Z",
     "start_time": "2023-07-06T08:18:14.152797051Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "class EIN(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0.0,\n",
    "        edge_dim=None,\n",
    "        train_eps = False,\n",
    "        eps = 0.0,\n",
    "        bias=True,\n",
    "        share_weights=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add' , **kwargs) # defines the aggregation method: `aggr='add'`\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.share_weights = share_weights\n",
    "        self.edge_dim = edge_dim\n",
    "        self.initial_eps = eps\n",
    "\n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        # For influence mechanism\n",
    "        self.inf = Linear(edge_dim, out_channels)\n",
    "\n",
    "        # Tunable parameter for adding self node features...\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None # alpha weights\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        self.inf.reset_parameters()\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l = None # for source nodes\n",
    "        x_r = None # for target nodes\n",
    "\n",
    "        x_l = self.lin_l(x).view(-1, H, C) # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        # Check the edge features shape: test_case\n",
    "        if edge_attr is not None:\n",
    "            print(f'edge_features shape: {edge_attr.shape}')\n",
    "        else:\n",
    "            print('No edge features!')\n",
    "\n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr, size=None) # (N, H_out)\n",
    "        # out += x_r.mean(dim=1) # add the self features\n",
    "\n",
    "        alpha = self._alpha # (#edges, 1)\n",
    "        assert alpha is not None\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        # Returning attention weights with computed hidden features\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if torch.is_tensor(edge_index):\n",
    "                return out, alpha.mean(dim=1, keepdims=True)\n",
    "        else:\n",
    "            return out # (N, H_out)\n",
    "\n",
    "    def message(self, x_j, x_i, index, size_i, edge_attr):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "\n",
    "        x = x_i + x_j # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1) # (#edges, NH)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i) # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha.mean(dim=1, keepdims=True) # (#edges, 1)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training) # randomly dropping attention during training\n",
    "        node_out = (x_j * alpha.unsqueeze(-1)).mean(dim=1)\n",
    "\n",
    "        tmp = scatter(node_out, index, dim=0 ,reduce='mean')\n",
    "        print('tmp', tmp.shape)\n",
    "        print(index, node_out.shape)\n",
    "        if self.inf is not None:\n",
    "            edge_attr = self.inf(self._alpha * edge_attr)\n",
    "            return node_out + edge_attr  # (#edges, H_out)\n",
    "        return node_out\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        print(aggr_out.shape)\n",
    "        aggr_out += x[1].mean(dim=1) # add the self features\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:30:17.298595145Z",
     "start_time": "2023-07-06T08:30:17.287926473Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_features shape: torch.Size([8, 5])\n",
      "tmp torch.Size([4, 32])\n",
      "tensor([1, 2, 3, 3, 0, 0, 0, 2]) torch.Size([8, 32])\n",
      "torch.Size([4, 32])\n",
      "Returned results:\n",
      "Output dim: torch.Size([4, 32])\n",
      "Alpha weights:  torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "ein = EIN(embed_size, 32, heads=10, edge_dim=5, train_eps=False)\n",
    "res = ein.forward(x, edge_index, edge_attr=x_edge,return_attention_weights=True)\n",
    "print('Returned results:')\n",
    "print('Output dim:', res[0].shape)\n",
    "print('Alpha weights: ', res[1].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T08:30:17.890262421Z",
     "start_time": "2023-07-06T08:30:17.862138762Z"
    }
   }
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
