{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch.nn.init import xavier_uniform_, zeros_\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, softmax\n",
    "from typing import Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EINv3(MessagePassing):\n",
    "    \"\"\"\n",
    "    A Edge featured attention based Graph Neural Network Layer for Graph Classification / Regression Tasks: V3\n",
    "    \n",
    "    Notes:\n",
    "        Fully Multi-head attention is implemented in this version, compared to previous versions where concatenation is ommited and mean of values are used.\n",
    "        In this case, mean value of attention is used only with influence mechanism!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            heads=1,\n",
    "            negative_slope=0.2,\n",
    "            dropout=0.0,\n",
    "            edge_dim=None,\n",
    "            train_eps=False,\n",
    "            eps=0.0,\n",
    "            bias=True,\n",
    "            share_weights=False,\n",
    "            concat=True,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add', **kwargs)  # defines the aggregation method: `aggr='add'`\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.share_weights = share_weights\n",
    "        self.edge_dim = edge_dim\n",
    "        self.initial_eps = eps\n",
    "        self.concat = concat\n",
    "\n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l  # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        # For influence mechanism\n",
    "        self.inf = Linear(edge_dim, out_channels)\n",
    "\n",
    "        # Tunable parameter for adding self node features...\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None  # alpha weights\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        self.inf.reset_parameters()\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l = None  # for source nodes\n",
    "        x_r = None  # for target nodes\n",
    "\n",
    "        x_l = self.lin_l(x).view(-1, H, C)  # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        # Check the edge features shape: test_case\n",
    "        # if edge_attr is not None:\n",
    "        #     print(f'edge_features shape: {edge_attr.shape}')\n",
    "        # else:\n",
    "        #     print('No edge features!')\n",
    "\n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr, size=None)  # (N, H_out)\n",
    "        # out += x_r.mean(dim=1) # add the self features\n",
    "\n",
    "        alpha = self._alpha  # (#edges, NH, H_out)\n",
    "        assert alpha is not None, 'Alpha weights can not be None value!'\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if self.concat == True: # (N,  NH * H_out), for alpha: (N, NH, H_out)\n",
    "            if isinstance(return_attention_weights, bool):\n",
    "                return out.view(-1, self.heads * self.out_channels), alpha\n",
    "            return out.view(-1, self.heads * self.out_channels)\n",
    "        \n",
    "        # Taking the mean of heads  -> (N, H_out)\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            return out.mean(dim=1), alpha.mean(dim=1)\n",
    "        return out.mean(dim=1), alpha.mean(dim=1)\n",
    "\n",
    "\n",
    "    def message(self, x_j, x_i, index, size_i, edge_attr):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "\n",
    "        x = x_i + x_j  # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att) # (#edges, NH, H_out)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i)  # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha  # (#edges, NH, H_out)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # randomly dropping attention during training\n",
    "        node_out = x_j * alpha\n",
    "\n",
    "        if self.inf is not None and edge_attr is not None:\n",
    "            if self.edge_dim != edge_attr.size(-1):\n",
    "                raise ValueError(\"Node and edge feature dimensionality do not \"\n",
    "                                 \"match. Consider setting the 'edge_dim' \"\"attribute\")\n",
    "            edge_attr = self.inf(self._alpha.mean(dim=-1) * edge_attr)  # transformed edge features via influence mechanism\n",
    "            return node_out + edge_attr.unsqueeze(1)  # (#edges, H_out)\n",
    "        return node_out  # (#edges, H_out)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        aggr_out += (1 + self.eps) * x[1]  # add the self features with a weighting factor\n",
    "        return aggr_out  # (N, H_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 32)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr = torch.randn(16, 8)\n",
    "edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.randint(1, 10, (2, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = EINv3(32, 100, 8, edge_dim=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 800]) torch.Size([16, 8, 100])\n"
     ]
    }
   ],
   "source": [
    "out1, out2 = conv1(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "print(out1.shape, out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 300]) torch.Size([16, 300])\n"
     ]
    }
   ],
   "source": [
    "conv2 = EINv3(out1.shape[-1], 300, 8, edge_dim=8, concat=False)\n",
    "\n",
    "out3, out4 = conv2(out1, edge_index, edge_attr, return_attention_weights=True)\n",
    "print(out3.shape, out4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 16, 50])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn(40, 16, 50)\n",
    "t2 = torch.randn(40, 50)\n",
    "\n",
    "t3 = t1 + t2.unsqueeze(1)\n",
    "t3.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
