{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch.nn.init import xavier_uniform_, zeros_\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, softmax\n",
    "from typing import Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EINv3(MessagePassing):\n",
    "    \"\"\"\n",
    "    A Edge featured attention based Graph Neural Network Layer for Graph Classification / Regression Tasks: V3\n",
    "    \n",
    "    Notes:\n",
    "        Fully Multi-head attention is implemented in this version, compared to previous versions where concatenation is ommited and mean of values are used.\n",
    "        In this case, mean value of attention is used only with influence mechanism!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            heads=1,\n",
    "            negative_slope=0.2,\n",
    "            dropout=0.0,\n",
    "            edge_dim=None,\n",
    "            train_eps=False,\n",
    "            eps=0.0,\n",
    "            bias=True,\n",
    "            share_weights=False,\n",
    "            concat=True,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add', **kwargs)  # defines the aggregation method: `aggr='add'`\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.share_weights = share_weights\n",
    "        self.edge_dim = edge_dim\n",
    "        self.initial_eps = eps\n",
    "        self.concat = concat\n",
    "\n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l  # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        # For influence mechanism\n",
    "        self.inf = Linear(edge_dim, out_channels)\n",
    "\n",
    "        # Tunable parameter for adding self node features...\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None  # alpha weights\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        self.inf.reset_parameters()\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l = None  # for source nodes\n",
    "        x_r = None  # for target nodes\n",
    "\n",
    "        x_l = self.lin_l(x).view(-1, H, C)  # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        # Check the edge features shape: test_case\n",
    "        # if edge_attr is not None:\n",
    "        #     print(f'edge_features shape: {edge_attr.shape}')\n",
    "        # else:\n",
    "        #     print('No edge features!')\n",
    "\n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr, size=None)  # (N, H_out)\n",
    "        # out += x_r.mean(dim=1) # add the self features\n",
    "        alpha = self._alpha  # (#edges, NH, H_out)\n",
    "        assert alpha is not None, 'Alpha weights can not be None value!'\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if self.concat == True: # (N,  NH * H_out), for alpha: (N, NH, H_out)\n",
    "            if isinstance(return_attention_weights, bool):\n",
    "                return out.view(-1, self.heads * self.out_channels), alpha\n",
    "            return out.view(-1, self.heads * self.out_channels)\n",
    "        \n",
    "        # Taking the mean of heads  -> (N, H_out)\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            return out.mean(dim=1), alpha.mean(dim=1)\n",
    "\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "\n",
    "    def message(self, x_j, x_i, index, size_i, edge_attr):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "\n",
    "        x = x_i + x_j  # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1) # (#edges, NH)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i)  # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha  # (#edges, NH)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # randomly dropping attention during training\n",
    "        node_out = x_j * alpha.unsqueeze(dim=-1)\n",
    "\n",
    "        if self.inf is not None and edge_attr is not None:\n",
    "            if self.edge_dim != edge_attr.size(-1):\n",
    "                raise ValueError(\"Node and edge feature dimensionality do not \"\n",
    "                                 \"match. Consider setting the 'edge_dim' \"\"attribute\")\n",
    "            edge_attr = self.inf(self._alpha.mean(dim=-1, keepdim=True) * edge_attr)  # transformed edge features via influence mechanism\n",
    "            return node_out + edge_attr.unsqueeze(1)  # (#edges, H_out)\n",
    "        return node_out  # (#edges, H_out)\n",
    "\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        aggr_out += (1 + self.eps) * x[1]  # add the self features with a weighting factor\n",
    "        return aggr_out  # (N, H_out)\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 32)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = torch.randn(16, 8)\n",
    "edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.randint(1, 10, (2, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = EINv3(32, 100, 8, edge_dim=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, out2 = conv1(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "print(out1.shape, out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = EINv3(out1.shape[-1], 300, 8, edge_dim=8, concat=False)\n",
    "\n",
    "out3, out4 = conv2(out1, edge_index, edge_attr, return_attention_weights=True)\n",
    "print(out3.shape, out4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(40, 16, 50)\n",
    "t2 = torch.randn(40, 50)\n",
    "\n",
    "t3 = t1 + t2.unsqueeze(1)\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../Libs')\n",
    "from models import *\n",
    "from dataloaders import create_dataloaders\n",
    "from train import train, run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset_name': 'MUTAG',\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "train_loader, val_loader, test_loader, metadata = create_dataloaders(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import MessagePassing, GCNConv, GATv2Conv, GINConv, GINEConv, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EINModel_v3(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dim_h, final_dim, num_heads, edge_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Layers\n",
    "        self.conv1 = EINv3(input_dim, dim_h,\n",
    "                               edge_dim=edge_dim, heads=num_heads, **kwargs)\n",
    "        self.conv2 = EINv3(dim_h * num_heads, dim_h,\n",
    "                               edge_dim=edge_dim, heads=num_heads, **kwargs)\n",
    "        self.conv3 = EINv3(dim_h * num_heads, dim_h, \n",
    "                               edge_dim=edge_dim, heads=num_heads, concat=False, **kwargs)\n",
    "\n",
    "        # Linear layer\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "\n",
    "        # Classification head\n",
    "        self.lin2 = Linear(dim_h * 3, final_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Embedding\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h1 = h1.relu()\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h2 = h2.relu()\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "        h3 = h3.relu()\n",
    "\n",
    "        C = h3.shape[-1]  # dim_h\n",
    "        H = h2.shape[-1] // C  # num_heads\n",
    "\n",
    "        # Graph-level readout\n",
    "        h1 = global_mean_pool(h1.view(-1, H, C).mean(dim=1), batch)\n",
    "        h2 = global_mean_pool(h2.view(-1, H, C).mean(dim=1), batch)\n",
    "        h3 = global_mean_pool(h3, batch)\n",
    "\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EINModel_v3(input_dim=7, dim_h=64, final_dim=2, edge_dim=4, num_heads=16, eps=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_tmp = EINv3(in_channels=7, out_channels=100, heads=16, edge_dim=4, concat=False)\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    res = conv_tmp(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "    print(res.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    res = model.forward(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "    print(res.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
