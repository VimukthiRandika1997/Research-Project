{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "\n",
    "dataset = MoleculeNet(root='../../Data/MoleculeNet', name='BACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "smiles_list = pd.read_csv('../../Data/MoleculeNet/bace/raw/bace.csv')['mol'].tolist()\n",
    "smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../Libs')\n",
    "from Libs.splitting import scaffold_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, task_idx=None, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    print(batch.edge_index)\n",
    "    print(degree(batch.edge_index[0]), '\\n')\n",
    "    print(degree(batch.edge_index[1]))\n",
    "    print(degree(batch.edge_index[1]).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = torch.tensor([0, 1, 0, 2, 0, 1])\n",
    "degree(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch.nn.init import xavier_uniform_, zeros_\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, softmax\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class EINv4(MessagePassing):\n",
    "    \"\"\"\n",
    "    A Edge featured attention based Graph Neural Network Layer for Graph Classification / Regression Tasks: V4\n",
    "\n",
    "    Note: \n",
    "        - Graph_token is added here which summerizes the whole the graph / batch\n",
    "        - Centrality encoding is added: in-degree, out-degree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            heads=1,\n",
    "            negative_slope=0.2,\n",
    "            dropout=0.0,\n",
    "            edge_dim=None,\n",
    "            train_eps=False,\n",
    "            eps=0.0,\n",
    "            bias=True,\n",
    "            share_weights=False,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add', **kwargs)  # defines the aggregation method: `aggr='add'`\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.share_weights = share_weights\n",
    "        self.edge_dim = edge_dim\n",
    "        self.initial_eps = eps\n",
    "\n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l  # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        # For influence mechanism\n",
    "        self.inf = Linear(edge_dim, out_channels)\n",
    "\n",
    "        # Tunable parameter for adding self node features...\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "        \n",
    "        # In-degree and Out-degree encoders\n",
    "        self.in_degree_encoder = nn.Embedding(out_channels, out_channels, padding_idx=0)\n",
    "        self.out_degree_encoder = nn.Embedding(out_channels, out_channels, padding_idx=0)\n",
    "\n",
    "        # Graph Token\n",
    "        self.graph_token = nn.Embedding(1, out_channels)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None  # alpha weights\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        self.inf.reset_parameters()\n",
    "        self.graph_token.reset_parameters()\n",
    "        self.in_degree_encoder.reset_parameters()\n",
    "        self.out_degree_encoder.reset_parameters()\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l = None  # for source nodes\n",
    "        x_r = None  # for target nodes\n",
    "\n",
    "        x_l = self.lin_l(x).view(-1, H, C)  # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr, size=None)  # (N, H_out)\n",
    "        # out += x_r.mean(dim=1) # add the self features\n",
    "\n",
    "        alpha = self._alpha  # (#edges, 1)\n",
    "        assert alpha is not None, 'Alpha weights can not be None value!'\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        # Add in-degree and out-degree informations\n",
    "        in_degree = degree(edge_index[1]).to(torch.long)\n",
    "        out_degree = degree(edge_index[0]).to(torch.long)\n",
    "        out = (\n",
    "            out \n",
    "            + self.in_degree_encoder(in_degree) \n",
    "            + self.out_degree_encoder(out_degree)\n",
    "        )\n",
    "\n",
    "        # Add graph-token information\n",
    "        # graph_token_feature = self.graph_token.weight.repeat(out.shape[0], 1)\n",
    "        graph_token_feature = self.graph_token.weight\n",
    "        out = out + graph_token_feature\n",
    "\n",
    "        # Returning attention weights with computed hidden features\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            return out, alpha.mean(dim=1, keepdims=True)\n",
    "        else:\n",
    "            return out  # (N, H_out)\n",
    "\n",
    "    def message(self, x_j, x_i, index, size_i, edge_attr):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "\n",
    "        x = x_i + x_j  # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1)  # (#edges, NH)\n",
    "        alpha = softmax(alpha, index,\n",
    "                        num_nodes=size_i)  # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha.mean(dim=1, keepdims=True)  # (#edges, 1)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # randomly dropping attention during training\n",
    "        node_out = (x_j * alpha.unsqueeze(-1)).mean(dim=1)\n",
    "\n",
    "        if self.inf is not None and edge_attr is not None:\n",
    "            if self.edge_dim != edge_attr.size(-1):\n",
    "                raise ValueError(\"Node and edge feature dimensionality do not \"\n",
    "                                 \"match. Consider setting the 'edge_dim' \"\"attribute\")\n",
    "            edge_attr = self.inf(self._alpha * edge_attr)  # transformed edge features via influence mechanism\n",
    "            return node_out + edge_attr  # (#edges, H_out)\n",
    "        return node_out  # (#edges, H_out)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        aggr_out += (1 + self.eps) * x[1].mean(dim=1)  # add the self features with a weighting factor\n",
    "        return aggr_out  # (N, H_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = EINv4(9, 64, 16, edge_dim=3)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(layer.forward(batch.x.to(torch.float), batch.edge_index, batch.edge_attr).shape)\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
