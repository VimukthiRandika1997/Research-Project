{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# splitter function\n",
    "\n",
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    \"\"\"\n",
    "    Obtain Bemis-Murcko scaffold from smiles\n",
    "    :param smiles:\n",
    "    :param include_chirality:\n",
    "    :return: smiles of scaffold\n",
    "    \"\"\"\n",
    "    scaffold = MurckoScaffold.MurckoScaffoldSmiles(\n",
    "        smiles=smiles, includeChirality=include_chirality)\n",
    "    return scaffold\n",
    "\n",
    "# # test generate_scaffold\n",
    "# s = 'Cc1cc(Oc2nccc(CCC)c2)ccc1'\n",
    "# scaffold = generate_scaffold(s)\n",
    "# assert scaffold == 'c1ccc(Oc2ccccn2)cc1'\n",
    "\n",
    "def scaffold_split(dataset, smiles_list, task_idx=None, null_value=0,\n",
    "                   frac_train=0.8, frac_valid=0.1, frac_test=0.1,\n",
    "                   return_smiles=False):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/deepchem/deepchem/blob/master/deepchem/splits/splitters.py\n",
    "    Split dataset by Bemis-Murcko scaffolds\n",
    "    This function can also ignore examples containing null values for a\n",
    "    selected task when splitting. Deterministic split\n",
    "    :param dataset: pytorch geometric dataset obj\n",
    "    :param smiles_list: list of smiles corresponding to the dataset obj\n",
    "    :param task_idx: column idx of the data.y tensor. Will filter out\n",
    "    examples with null value in specified task column of the data.y tensor\n",
    "    prior to splitting. If None, then no filtering\n",
    "    :param null_value: float that specifies null value in data.y to filter if\n",
    "    task_idx is provided\n",
    "    :param frac_train:\n",
    "    :param frac_valid:\n",
    "    :param frac_test:\n",
    "    :param return_smiles:\n",
    "    :return: train, valid, test slices of the input dataset obj. If\n",
    "    return_smiles = True, also returns ([train_smiles_list],\n",
    "    [valid_smiles_list], [test_smiles_list])\n",
    "    \"\"\"\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.0)\n",
    "\n",
    "    if task_idx != None:\n",
    "        # filter based on null values in task_idx\n",
    "        # get task array\n",
    "        y_task = np.array([data.y[task_idx].item() for data in dataset])\n",
    "        # boolean array that correspond to non null values\n",
    "        non_null = y_task != null_value\n",
    "        smiles_list = list(compress(enumerate(smiles_list), non_null))\n",
    "    else:\n",
    "        non_null = np.ones(len(dataset)) == 1\n",
    "        smiles_list = list(compress(enumerate(smiles_list), non_null))\n",
    "\n",
    "    # create dict of the form {scaffold_i: [idx1, idx....]}\n",
    "    all_scaffolds = {}\n",
    "    for i, smiles in smiles_list:\n",
    "        scaffold = generate_scaffold(smiles, include_chirality=True)\n",
    "        if scaffold not in all_scaffolds:\n",
    "            all_scaffolds[scaffold] = [i]\n",
    "        else:\n",
    "            all_scaffolds[scaffold].append(i)\n",
    "\n",
    "    # sort from largest to smallest sets\n",
    "    all_scaffolds = {key: sorted(value) for key, value in all_scaffolds.items()}\n",
    "    all_scaffold_sets = [\n",
    "        scaffold_set for (scaffold, scaffold_set) in sorted(\n",
    "            all_scaffolds.items(), key=lambda x: (len(x[1]), x[1][0]), reverse=True)\n",
    "    ]\n",
    "\n",
    "    # get train, valid test indices\n",
    "    train_cutoff = frac_train * len(smiles_list)\n",
    "    valid_cutoff = (frac_train + frac_valid) * len(smiles_list)\n",
    "    train_idx, valid_idx, test_idx = [], [], []\n",
    "    for scaffold_set in all_scaffold_sets:\n",
    "        if len(train_idx) + len(scaffold_set) > train_cutoff:\n",
    "            if len(train_idx) + len(valid_idx) + len(scaffold_set) > valid_cutoff:\n",
    "                test_idx.extend(scaffold_set)\n",
    "            else:\n",
    "                valid_idx.extend(scaffold_set)\n",
    "        else:\n",
    "            train_idx.extend(scaffold_set)\n",
    "\n",
    "    assert len(set(train_idx).intersection(set(valid_idx))) == 0\n",
    "    assert len(set(test_idx).intersection(set(valid_idx))) == 0\n",
    "\n",
    "    train_dataset = dataset[torch.tensor(train_idx)]\n",
    "    valid_dataset = dataset[torch.tensor(valid_idx)]\n",
    "    test_dataset = dataset[torch.tensor(test_idx)]\n",
    "\n",
    "    if not return_smiles:\n",
    "        return train_dataset, valid_dataset, test_dataset\n",
    "    else:\n",
    "        train_smiles = [smiles_list[i][1] for i in train_idx]\n",
    "        valid_smiles = [smiles_list[i][1] for i in valid_idx]\n",
    "        test_smiles = [smiles_list[i][1] for i in test_idx]\n",
    "        return train_dataset, valid_dataset, test_dataset, (train_smiles,\n",
    "                                                            valid_smiles,\n",
    "                                                            test_smiles)\n",
    "\n",
    "def random_scaffold_split(dataset, smiles_list, task_idx=None, null_value=0,\n",
    "                   frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=0):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/pfnet-research/chainer-chemistry/blob/master/chainer_chemistry/dataset/splitters/scaffold_splitter.py\n",
    "    Split dataset by Bemis-Murcko scaffolds\n",
    "    This function can also ignore examples containing null values for a\n",
    "    selected task when splitting. Deterministic split\n",
    "    :param dataset: pytorch geometric dataset obj\n",
    "    :param smiles_list: list of smiles corresponding to the dataset obj\n",
    "    :param task_idx: column idx of the data.y tensor. Will filter out\n",
    "    examples with null value in specified task column of the data.y tensor\n",
    "    prior to splitting. If None, then no filtering\n",
    "    :param null_value: float that specifies null value in data.y to filter if\n",
    "    task_idx is provided\n",
    "    :param frac_train:\n",
    "    :param frac_valid:\n",
    "    :param frac_test:\n",
    "    :param seed;\n",
    "    :return: train, valid, test slices of the input dataset obj\n",
    "    \"\"\"\n",
    "\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.0)\n",
    "\n",
    "    if task_idx != None:\n",
    "        # filter based on null values in task_idx\n",
    "        # get task array\n",
    "        y_task = np.array([data.y[task_idx].item() for data in dataset])\n",
    "        # boolean array that correspond to non null values\n",
    "        non_null = y_task != null_value\n",
    "        smiles_list = list(compress(enumerate(smiles_list), non_null))\n",
    "    else:\n",
    "        non_null = np.ones(len(dataset)) == 1\n",
    "        smiles_list = list(compress(enumerate(smiles_list), non_null))\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    scaffolds = defaultdict(list)\n",
    "    for ind, smiles in smiles_list:\n",
    "        scaffold = generate_scaffold(smiles, include_chirality=True)\n",
    "        scaffolds[scaffold].append(ind)\n",
    "\n",
    "    scaffold_sets = rng.permutation(list(scaffolds.values()))\n",
    "\n",
    "    n_total_valid = int(np.floor(frac_valid * len(dataset)))\n",
    "    n_total_test = int(np.floor(frac_test * len(dataset)))\n",
    "\n",
    "    train_idx = []\n",
    "    valid_idx = []\n",
    "    test_idx = []\n",
    "\n",
    "    for scaffold_set in scaffold_sets:\n",
    "        if len(valid_idx) + len(scaffold_set) <= n_total_valid:\n",
    "            valid_idx.extend(scaffold_set)\n",
    "        elif len(test_idx) + len(scaffold_set) <= n_total_test:\n",
    "            test_idx.extend(scaffold_set)\n",
    "        else:\n",
    "            train_idx.extend(scaffold_set)\n",
    "\n",
    "    train_dataset = dataset[torch.tensor(train_idx)]\n",
    "    valid_dataset = dataset[torch.tensor(valid_idx)]\n",
    "    test_dataset = dataset[torch.tensor(test_idx)]\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "def random_split(dataset, task_idx=None, null_value=0,\n",
    "                   frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=0,\n",
    "                 smiles_list=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param dataset:\n",
    "    :param task_idx:\n",
    "    :param null_value:\n",
    "    :param frac_train:\n",
    "    :param frac_valid:\n",
    "    :param frac_test:\n",
    "    :param seed:\n",
    "    :param smiles_list: list of smiles corresponding to the dataset obj, or None\n",
    "    :return: train, valid, test slices of the input dataset obj. If\n",
    "    smiles_list != None, also returns ([train_smiles_list],\n",
    "    [valid_smiles_list], [test_smiles_list])\n",
    "    \"\"\"\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.0)\n",
    "\n",
    "    if task_idx != None:\n",
    "        # filter based on null values in task_idx\n",
    "        # get task array\n",
    "        y_task = np.array([data.y[task_idx].item() for data in dataset])\n",
    "        non_null = y_task != null_value  # boolean array that correspond to non null values\n",
    "        idx_array = np.where(non_null)[0]\n",
    "        dataset = dataset[torch.tensor(idx_array)]  # examples containing non\n",
    "        # null labels in the specified task_idx\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    num_mols = len(dataset)\n",
    "    random.seed(seed)\n",
    "    all_idx = list(range(num_mols))\n",
    "    random.shuffle(all_idx)\n",
    "\n",
    "    train_idx = all_idx[:int(frac_train * num_mols)]\n",
    "    valid_idx = all_idx[int(frac_train * num_mols):int(frac_valid * num_mols)\n",
    "                                                   + int(frac_train * num_mols)]\n",
    "    test_idx = all_idx[int(frac_valid * num_mols) + int(frac_train * num_mols):]\n",
    "\n",
    "    assert len(set(train_idx).intersection(set(valid_idx))) == 0\n",
    "    assert len(set(valid_idx).intersection(set(test_idx))) == 0\n",
    "    assert len(train_idx) + len(valid_idx) + len(test_idx) == num_mols\n",
    "\n",
    "    train_dataset = dataset[torch.tensor(train_idx)]\n",
    "    valid_dataset = dataset[torch.tensor(valid_idx)]\n",
    "    test_dataset = dataset[torch.tensor(test_idx)]\n",
    "\n",
    "    if not smiles_list:\n",
    "        return train_dataset, valid_dataset, test_dataset\n",
    "    else:\n",
    "        train_smiles = [smiles_list[i] for i in train_idx]\n",
    "        valid_smiles = [smiles_list[i] for i in valid_idx]\n",
    "        test_smiles = [smiles_list[i] for i in test_idx]\n",
    "        return train_dataset, valid_dataset, test_dataset, (train_smiles,\n",
    "                                                            valid_smiles,\n",
    "                                                            test_smiles)\n",
    "\n",
    "\n",
    "def cv_random_split(dataset, fold_idx = 0,\n",
    "                   frac_train=0.9, frac_valid=0.1, seed=0,\n",
    "                 smiles_list=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param dataset:\n",
    "    :param task_idx:\n",
    "    :param null_value:\n",
    "    :param frac_train:\n",
    "    :param frac_valid:\n",
    "    :param frac_test:\n",
    "    :param seed:\n",
    "    :param smiles_list: list of smiles corresponding to the dataset obj, or None\n",
    "    :return: train, valid, test slices of the input dataset obj. If\n",
    "    smiles_list != None, also returns ([train_smiles_list],\n",
    "    [valid_smiles_list], [test_smiles_list])\n",
    "    \"\"\"\n",
    "\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid, 1.0)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = seed)\n",
    "\n",
    "    labels = [data.y.item() for data in dataset]\n",
    "\n",
    "    idx_list = []\n",
    "\n",
    "    for idx in skf.split(np.zeros(len(labels)), labels):\n",
    "        idx_list.append(idx)\n",
    "    train_idx, val_idx = idx_list[fold_idx]\n",
    "\n",
    "    train_dataset = dataset[torch.tensor(train_idx)]\n",
    "    valid_dataset = dataset[torch.tensor(val_idx)]\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "\n",
    "dataset = MoleculeNet(root='dataset/MoleculeNet', name='Tox21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "smiles_list = pd.read_csv('dataset/MoleculeNet/tox21/raw/tox21.csv')['smiles'].tolist()\n",
    "smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, task_idx=None, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
