{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6cc11b-bcde-4190-833b-478becf690df",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "abd9a52e-cba6-43fa-9514-a40406048ffe",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "# Pytorch Geometric Workflow...\n",
    "\n",
    "- `__check_input__(**kwargs)`: check **SparseTensor** or not\n",
    "- `__collect__(**kwargs)`: Contruct the message of **node i**, for every node in the graph\n",
    "- `message(**kwargs)`: construct the **node i**'s message\n",
    "- `aggregate(**kwargs)`: message aggregation: max, min, mean, add\n",
    "- `update(**kwargs)`: update the features with aggregated message + current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576941c0-bb20-4d74-a0e3-7f50967b0484",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "1b635303-29ed-4998-a5bb-583c7c3f3a2b",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:37.601910632Z",
     "start_time": "2023-07-09T10:17:33.815218750Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch.nn.init import xavier_uniform_, zeros_\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00640a7-1457-411c-a5e8-bae324918c2b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "0bda0e28-b479-4c2b-9e2d-576468c8437e",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:37.604523683Z",
     "start_time": "2023-07-09T10:17:37.599534478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features\n",
      " tensor([[0.4753, 0.7336, 0.3778, 0.8182, 0.1771],\n",
      "        [0.0397, 0.9440, 0.5285, 0.9309, 0.2452],\n",
      "        [0.2214, 0.1756, 0.7032, 0.9054, 0.5184],\n",
      "        [0.2881, 0.5517, 0.6869, 0.7084, 0.4849]]) torch.Size([4, 5])\n",
      "\n",
      "edge_index\n",
      " tensor([[0, 0, 0, 1, 1, 2, 3, 3],\n",
      "        [1, 2, 3, 3, 0, 0, 0, 2]]) torch.Size([2, 8])\n",
      "\n",
      "Edge features\n",
      " tensor([[0.5982, 0.9028, 0.9854, 0.0744, 0.2471],\n",
      "        [0.8321, 0.6367, 0.5588, 0.7835, 0.9423],\n",
      "        [0.5290, 0.4307, 0.8699, 0.4362, 0.2070],\n",
      "        [0.6127, 0.0396, 0.9917, 0.9920, 0.9192],\n",
      "        [0.8113, 0.2669, 0.9257, 0.5417, 0.9243],\n",
      "        [0.0989, 0.0144, 0.2029, 0.5586, 0.4973],\n",
      "        [0.6190, 0.5799, 0.8756, 0.0480, 0.0155],\n",
      "        [0.7155, 0.2190, 0.7044, 0.6091, 0.3158]]) torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 4 # number of nodes in the graph\n",
    "embed_size = 5 # initial node features\n",
    "\n",
    "# Creating node features\n",
    "node_feat = torch.rand((num_nodes, embed_size), dtype=torch.float)\n",
    "x = node_feat\n",
    "print('Node features\\n', node_feat, node_feat.shape)\n",
    "\n",
    "# Creating COO format edge_indexes for the graph\n",
    "src_index = torch.tensor([0,0,0,1,1,2,3,3], dtype=torch.long)\n",
    "target_index = torch.tensor([1,2,3,3,0,0,0,2], dtype=torch.long)\n",
    "edge_index = torch.zeros(size=(2, src_index.size()[0]), dtype=torch.int64)\n",
    "edge_index[0] = src_index\n",
    "edge_index[1] = target_index\n",
    "print('\\nedge_index\\n', edge_index, edge_index.shape)\n",
    "\n",
    "# Creating edge features\n",
    "x_edge = torch.rand((edge_index.shape[1], 5), dtype=torch.float) # (num_edges, embed_size)\n",
    "print('\\nEdge features\\n', x_edge, x_edge.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df567d0d-d8ea-44d6-91bd-080819cec67c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "5fe7a8d9-0991-475e-b10a-3bff3588b843",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623a4f2f-811f-4f2e-91fd-5030cc52975c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "36693d76-218a-4761-94b0-0707bda598eb",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:37.798975381Z",
     "start_time": "2023-07-09T10:17:37.600347864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100]) torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "gat = GATv2Conv(5, 100)\n",
    "res1, res2 = gat.forward(x, edge_index, return_attention_weights=True)\n",
    "print(res1[0].shape, res2[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279880b8-60f1-4911-a1d5-351739917288",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "561c961b-1b6c-4128-992d-8a053c63ef31",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:37.856889355Z",
     "start_time": "2023-07-09T10:17:37.792534287Z"
    }
   },
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, \n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        concat=True,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0.0,\n",
    "        add_self_loops=True,\n",
    "        bias=True,\n",
    "        share_weights=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add' , **kwargs) # defines the aggregation method: `aggr='add'`\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.share_weights = share_weights\n",
    "        \n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias,)\n",
    "        \n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "            \n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        \n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self._alpha = None\n",
    "\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)        \n",
    "        \n",
    "    def forward(self, x, edge_index, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "        \n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        x_l = None # for source nodes\n",
    "        x_r = None # for target nodes\n",
    "        \n",
    "        x_l = self.lin_l(x).view(-1, H, C) # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "            \n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "        \n",
    "        if self.add_self_loops: # Adding self-loops for the graph...\n",
    "            if torch.is_tensor(edge_index):\n",
    "                num_nodes = x_l.size(0)\n",
    "                if x_r is not None:\n",
    "                    num_nodes = min(num_nodes, x_r.size(0))\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "                \n",
    "        # Start propagating info...: contruct message -> aggreate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), size=None) # (N, NH, H_out)\n",
    "        \n",
    "        alpha = self._alpha # (#edges, NH)\n",
    "        assert alpha is not None\n",
    "        self._alpha = None\n",
    "        \n",
    "        if self.concat: # (N, NH, H_out) -> (N, NH * H_out)\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1) # (N, NH, H_out) -> (N, H_out)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        \n",
    "            \n",
    "        # Returning attetion weights with computed hidden features    \n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if torch.is_tensor(edge_index):\n",
    "                return out, (edge_index, alpha) # Depends on 'concat', ((2, #edges), (#edges, NH))\n",
    "        else:   \n",
    "            return out # Depends on 'concat': (N, NH * H_out) || (N, H_out)\n",
    "    \n",
    "    def message(self, x_j, x_i, index, size_i):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "        \n",
    "        x = x_i + x_j # adding(element-wise) source and target node features together to calculate atttention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i) # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha # (#edges, NH)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training) # randomly dropping attention during training\n",
    "        \n",
    "        return x_j * alpha.unsqueeze(-1) # (#edges, NH, H_out)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d197fb8-a7c1-4cee-9e4a-14a5d667a794",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "b656e0db-2ff0-4741-a0df-1950ba7ee8c0",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:37.965699724Z",
     "start_time": "2023-07-09T10:17:37.833039280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.1608, -0.1138, -0.3489,  0.0903, -0.0670, -0.1132, -0.0971,  0.1186,\n           0.0760,  0.1189],\n         [ 0.1312, -0.0939, -0.3707,  0.0681, -0.0256, -0.1286, -0.0869,  0.1697,\n           0.0737,  0.1206],\n         [ 0.1678, -0.1403, -0.3451,  0.0870, -0.0806, -0.0964, -0.1052,  0.0977,\n           0.0813,  0.1112],\n         [ 0.1531, -0.1009, -0.3622,  0.0824, -0.0441, -0.1213, -0.0989,  0.1488,\n           0.0703,  0.1154]], grad_fn=<AddBackward0>),\n (tensor([[0, 0, 0, 1, 1, 2, 3, 3, 0, 1, 2, 3],\n          [1, 2, 3, 3, 0, 0, 0, 2, 0, 1, 2, 3]]),\n  tensor([[0.5167, 0.5093, 0.5177, 0.4917, 0.4983, 0.4948, 0.5194, 0.4951],\n          [0.3298, 0.3368, 0.3535, 0.3341, 0.3303, 0.3526, 0.3467, 0.3229],\n          [0.3392, 0.3388, 0.3487, 0.3305, 0.3318, 0.3375, 0.3470, 0.3269],\n          [0.3178, 0.3293, 0.3293, 0.3375, 0.3312, 0.3504, 0.3237, 0.3322],\n          [0.2365, 0.2473, 0.2593, 0.2583, 0.2515, 0.2706, 0.2408, 0.2488],\n          [0.2568, 0.2492, 0.2154, 0.2435, 0.2473, 0.2296, 0.2477, 0.2555],\n          [0.2541, 0.2468, 0.2471, 0.2468, 0.2514, 0.2374, 0.2512, 0.2531],\n          [0.3337, 0.3299, 0.3371, 0.3335, 0.3354, 0.3281, 0.3289, 0.3369],\n          [0.2526, 0.2567, 0.2783, 0.2514, 0.2498, 0.2624, 0.2603, 0.2426],\n          [0.4833, 0.4907, 0.4823, 0.5083, 0.5017, 0.5052, 0.4806, 0.5049],\n          [0.3365, 0.3332, 0.3094, 0.3324, 0.3343, 0.3193, 0.3243, 0.3402],\n          [0.3430, 0.3319, 0.3219, 0.3320, 0.3370, 0.3121, 0.3293, 0.3409]],\n         grad_fn=<DifferentiableGraphBackward>)))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat = GAT(embed_size, 10, heads=8, concat=False, share_weights=True)\n",
    "gat.forward(x, edge_index, return_attention_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ca2ad-135e-4276-bf9c-0746e62247dc",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "38fc3fda-2866-44f6-b895-8385e84b9de7",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9ebc33-bdcf-4b45-9da7-153c67143632",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "df502a79-e59d-44b9-a896-45a32e6c3813",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:37.968840248Z",
     "start_time": "2023-07-09T10:17:37.885652906Z"
    }
   },
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, \n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        concat=True,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0.0,\n",
    "        add_self_loops=True,\n",
    "        bias=True,\n",
    "        share_weights=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add' , **kwargs) # defines the aggregation method: `aggr='add'`\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.share_weights = share_weights\n",
    "        \n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias,)\n",
    "        \n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "            \n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels * 2))\n",
    "        \n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self._alpha = None\n",
    "\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)        \n",
    "        \n",
    "    def forward(self, x, edge_index, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "        \n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        x_src = None # for source nodes\n",
    "        x_dst = None # for target nodes\n",
    "        \n",
    "        x_src = self.lin_l(x).view(-1, H, C) # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_dst = x_src\n",
    "        else:\n",
    "            x_dst = self.lin_r(x).view(-1, H, C)\n",
    "            \n",
    "        assert x_src is not None\n",
    "        assert x_dst is not None\n",
    "        \n",
    "        x = (x_src, x_dst)\n",
    "        a_src = x_src\n",
    "        a_dst = x_dst\n",
    "        a = (a_src, a_dst)\n",
    "        \n",
    "        if self.add_self_loops: # Adding self-loops for the graph...\n",
    "            if torch.is_tensor(edge_index):\n",
    "                num_nodes = x_src.size(0)\n",
    "                if x_dst is not None:\n",
    "                    num_nodes = min(num_nodes, x_dst.size(0))\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "                \n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=x, a=a, size=None) # (N, NH, H_out)\n",
    "        \n",
    "        alpha = self._alpha # (#edges, NH)\n",
    "        assert alpha is not None\n",
    "        self._alpha = None\n",
    "        \n",
    "        if self.concat: # (N, NH, H_out) -> (N, NH * H_out)\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1) # (N, NH, H_out) -> (N, H_out)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        \n",
    "            \n",
    "        # Returning attention weights with computed hidden features\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if torch.is_tensor(edge_index):\n",
    "                return out, (edge_index, alpha) # Depends on 'concat', ((2, #edges), (#edges, NH))\n",
    "        else:   \n",
    "            return out # Depends on 'concat': (N, NH * H_out) || (N, H_out)\n",
    "    \n",
    "    def message(self, x_j, x_i, a_i, a_j, index, size_i):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "        # print(a_i.shape, a_j.shape)\n",
    "        \n",
    "        # x = x_i + x_j # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = torch.cat([x_i, x_j], dim=-1)\n",
    "        # print(x.shape)\n",
    "        # print(self.att.shape)\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1)\n",
    "        # print(alpha.shape)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i) # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha # (#edges, NH)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training) # randomly dropping attention during training\n",
    "        \n",
    "        return x_j * alpha.unsqueeze(-1) # (#edges, NH, H_out)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda3b04f-b370-4e53-87fa-ed18b1f0c378",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "329bd66f-10dc-47bf-83f8-f28c551dd9b8",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:38.090733699Z",
     "start_time": "2023-07-09T10:17:37.952634122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "torch.Size([12, 8])\n",
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "gat = GAT(embed_size, 10, heads=8, concat=False, share_weights=True)\n",
    "res = gat.forward(x, edge_index, return_attention_weights=True)\n",
    "out, edge_index_returned_with_attention_weights = res[0], res[1]\n",
    "edge_index_returned, attention_weights = edge_index_returned_with_attention_weights\n",
    "print(out.shape)\n",
    "print(attention_weights.shape)\n",
    "print(edge_index_returned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# My Arch 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class EIN(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        concat=True,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0.0,\n",
    "        add_self_loops=True,\n",
    "        edge_dim=None,\n",
    "        fill_value='mean',\n",
    "        bias=True,\n",
    "        share_weights=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add' , **kwargs) # defines the aggregation method: `aggr='add'`\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.share_weights = share_weights\n",
    "        self.edge_dim = edge_dim\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        # For influence mechanism\n",
    "        # self.inf = Linear(edge_dim, out_channels)\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l = None # for source nodes\n",
    "        x_r = None # for target nodes\n",
    "\n",
    "        x_l = self.lin_l(x).view(-1, H, C) # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        if self.add_self_loops: # Adding self-loops for the graph...\n",
    "            if torch.is_tensor(edge_index):\n",
    "                if edge_attr is not None: # edge_attr is available -> add edge_attr for newly created self-loops: default `mean`\n",
    "                    num_nodes = x_l.size(0)\n",
    "                    if x_r is not None:\n",
    "                        num_nodes = min(num_nodes, x_r.size(0))\n",
    "                    edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "                    edge_index, edge_attr = add_self_loops(edge_index, edge_attr, fill_value=self.fill_value, num_nodes=num_nodes)\n",
    "                else:\n",
    "                    num_nodes = x_l.size(0)\n",
    "                    if x_r is not None:\n",
    "                        num_nodes = min(num_nodes, x_r.size(0))\n",
    "                    edge_index, _ = remove_self_loops(edge_index)\n",
    "                    edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "\n",
    "        # Check the edge features shape: test_case\n",
    "        if edge_attr is not None:\n",
    "            print(f'edge_features shape: {edge_attr.shape}')\n",
    "        else:\n",
    "            print('No edge features!')\n",
    "\n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr, size=None) # (N, NH, H_out)\n",
    "\n",
    "        alpha = self._alpha # (#edges, NH)\n",
    "        assert alpha is not None\n",
    "        self._alpha = None\n",
    "\n",
    "        if self.concat: # (N, NH, H_out) -> (N, NH * H_out)\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1) # (N, NH, H_out) -> (N, H_out)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        # Influence mechanism\n",
    "        res = alpha.mean(dim=1, keepdims=True) * edge_attr\n",
    "        print('inf: ', res.shape)\n",
    "\n",
    "        # Returning attention weights with computed hidden features\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if torch.is_tensor(edge_index):\n",
    "                if self.concat:\n",
    "                    return out, (edge_index, alpha) # Depends on 'concat', ((2, #edges), (#edges, NH))\n",
    "                return out, (edge_index, alpha.mean(dim=1, keepdims=True))\n",
    "        else:\n",
    "            return out # Depends on 'concat': (N, NH * H_out) || (N, H_out)\n",
    "\n",
    "    def message(self, x_j, x_i, index, size_i):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "\n",
    "        x = x_i + x_j # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1) # (#edges, NH)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i) # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha # (#edges, NH)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training) # randomly dropping attention during training\n",
    "\n",
    "        return x_j * alpha.unsqueeze(-1) # (#edges, NH, H_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:38.179072435Z",
     "start_time": "2023-07-09T10:17:38.092008415Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_features shape: torch.Size([12, 5])\n",
      "inf:  torch.Size([12, 5])\n",
      "torch.Size([4, 10])\n",
      "tensor([[0.4930],\n",
      "        [0.3335],\n",
      "        [0.3300],\n",
      "        [0.3398],\n",
      "        [0.2557],\n",
      "        [0.2482],\n",
      "        [0.2483],\n",
      "        [0.3332],\n",
      "        [0.2477],\n",
      "        [0.5070],\n",
      "        [0.3332],\n",
      "        [0.3302]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0, 0, 0, 1, 1, 2, 3, 3, 0, 1, 2, 3],\n",
      "        [1, 2, 3, 3, 0, 0, 0, 2, 0, 1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "ein = EIN(embed_size, 10, heads=8, concat=False,)\n",
    "res = ein.forward(x, edge_index, edge_attr=x_edge,return_attention_weights=True)\n",
    "out, r = res\n",
    "# print(r[1].shape)\n",
    "print(out.shape)\n",
    "print(r[1])\n",
    "print(r[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:38.220323441Z",
     "start_time": "2023-07-09T10:17:38.134617570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 5])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_edge.shape\n",
    "r[1][:8].shape\n",
    "# r[1][:8].reshape(8, 1) * x_edge\n",
    "temp = r[1][:8] * x_edge\n",
    "temp.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:38.247468747Z",
     "start_time": "2023-07-09T10:17:38.156601157Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from torch_scatter import gather_csr, scatter, segment_csr\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T10:17:38.248087049Z",
     "start_time": "2023-07-09T10:17:38.180185956Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class EIN(MessagePassing):\n",
    "    \"\"\"\n",
    "    A Edge featured attention based Graph Neural Network Layer for Graph Classification / Regression Tasks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0.0,\n",
    "        edge_dim=None,\n",
    "        train_eps = False,\n",
    "        eps = 0.0,\n",
    "        bias=True,\n",
    "        share_weights=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add' , **kwargs) # defines the aggregation method: `aggr='add'`\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.share_weights = share_weights\n",
    "        self.edge_dim = edge_dim\n",
    "        self.initial_eps = eps\n",
    "\n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        # For influence mechanism\n",
    "        self.inf = Linear(edge_dim, out_channels)\n",
    "\n",
    "        # Tunable parameter for adding self node features...\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None # alpha weights\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        self.inf.reset_parameters()\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l = None # for source nodes\n",
    "        x_r = None # for target nodes\n",
    "\n",
    "        x_l = self.lin_l(x).view(-1, H, C) # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        # Check the edge features shape: test_case\n",
    "        if edge_attr is not None:\n",
    "            print(f'edge_features shape: {edge_attr.shape}')\n",
    "        else:\n",
    "            print('No edge features!')\n",
    "\n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr, size=None) # (N, H_out)\n",
    "        # out += x_r.mean(dim=1) # add the self features\n",
    "\n",
    "        alpha = self._alpha # (#edges, 1)\n",
    "        assert alpha is not None, 'Alpha weights can not be None value!'\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        # Returning attention weights with computed hidden features\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "                return out, alpha.mean(dim=1, keepdims=True)\n",
    "        else:\n",
    "            return out # (N, H_out)\n",
    "\n",
    "\n",
    "    def message(self, x_j, x_i, index, size_i, edge_attr):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "\n",
    "        x = x_i + x_j # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1) # (#edges, NH)\n",
    "        alpha = softmax(alpha, index, num_nodes=size_i) # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha.mean(dim=1, keepdims=True) # (#edges, 1)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training) # randomly dropping attention during training\n",
    "        node_out = (x_j * alpha.unsqueeze(-1)).mean(dim=1)\n",
    "\n",
    "\n",
    "        if self.inf is not None and edge_attr is not None:\n",
    "            if self.edge_dim != edge_attr.size(-1):\n",
    "                raise ValueError(\"Node and edge feature dimensionalities do not \"\n",
    "                                \"match. Consider setting the 'edge_dim' \"\"attribute\")\n",
    "            edge_attr = self.inf(self._alpha * edge_attr) # transformed edge features via influence mechanism\n",
    "            return node_out + edge_attr  # (#edges, H_out)\n",
    "        return node_out # (#edges, H_out)\n",
    "\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        aggr_out += (1 + self.eps) * x[1].mean(dim=1) # add the self features with a weighting factor\n",
    "        return aggr_out # (N, H_out)\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T10:33:14.208308515Z",
     "start_time": "2023-07-09T10:33:14.162469547Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_features shape: torch.Size([8, 5])\n",
      "Returned results:\n",
      "torch.Size([4, 32]) torch.Size([8, 1])\n",
      "==========================================================================\n",
      "edge_features shape: torch.Size([8, 5])\n",
      "Returned results:\n",
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "ein = EIN(embed_size, 32, heads=10, edge_dim=5, train_eps=True)\n",
    "res = ein.forward(x, edge_index, edge_attr=x_edge, return_attention_weights=True)\n",
    "print('Returned results:')\n",
    "print(res[0].shape, res[1].shape)\n",
    "print('==========================================================================')\n",
    "\n",
    "ein2 = EIN(32, 64, heads=10, edge_dim=5, train_eps=True)\n",
    "res2 = ein2.forward(res[0], edge_index, edge_attr=x_edge)\n",
    "print('Returned results:')\n",
    "print(res2.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T10:34:22.737163015Z",
     "start_time": "2023-07-09T10:34:22.715442961Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How aggregation Work\n",
    "- out: source node messages, ex: [8, 32] -> [#edges, H_out]\n",
    "- index: target nodes indexes\n",
    "- dim: along the specified dimension, in this case dim = 0 (row-wise)\n",
    "\n",
    "After that the result of this function will be sent to the `update` function!\n",
    "\n",
    "    def aggregate(self, out, index):\n",
    "        return scatter(out, index, dim=0, reduce='mean')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
