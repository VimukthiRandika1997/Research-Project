{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../Libs')\n",
    "from models import *\n",
    "from dataloaders import create_dataloaders\n",
    "from train import train, run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset_name': 'BBBP',\n",
    "    'batch_size': 64,\n",
    "    'split_type': 'random'\n",
    "}\n",
    "\n",
    "train_loader, val_loader, test_loader, metadata = create_dataloaders(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import MessagePassing, GCNConv, GATv2Conv, GINConv, GINEConv, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EINv4(MessagePassing):\n",
    "    \"\"\"\n",
    "    A Edge featured attention based Graph Neural Network Layer for Graph Classification / Regression Tasks: V4\n",
    "\n",
    "    Note: \n",
    "        - Centrality encoding is added: in-degree, out-degree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            heads=1,\n",
    "            negative_slope=0.2,\n",
    "            dropout=0.0,\n",
    "            edge_dim=None,\n",
    "            train_eps=False,\n",
    "            eps=0.0,\n",
    "            bias=True,\n",
    "            share_weights=False,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, aggr='add', **kwargs)  # defines the aggregation method: `aggr='add'`\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.share_weights = share_weights\n",
    "        self.edge_dim = edge_dim\n",
    "        self.initial_eps = eps\n",
    "\n",
    "        # Linear Transformation\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l  # use same matrix\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # For attention calculation\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        # For influence mechanism\n",
    "        self.inf = Linear(edge_dim, out_channels)\n",
    "\n",
    "        # Tunable parameter for adding self node features...\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "        \n",
    "        # In-degree and Out-degree encoders\n",
    "        self.in_degree_encoder = nn.Embedding(out_channels, out_channels, padding_idx=0)\n",
    "        self.out_degree_encoder = nn.Embedding(out_channels, out_channels, padding_idx=0)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None  # alpha weights\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        self.inf.reset_parameters()\n",
    "        self.in_degree_encoder.reset_parameters()\n",
    "        self.out_degree_encoder.reset_parameters()\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "        xavier_uniform_(self.att)\n",
    "        zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        ## N - no_of_nodes, NH - no_of heads,  H_in - input_channels, H_out - out_channels\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l = None  # for source nodes\n",
    "        x_r = None  # for target nodes\n",
    "\n",
    "        x_l = self.lin_l(x).view(-1, H, C)  # (N, H_in) -> (N, NH, H_Out)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        # Start propagating info...: construct message -> aggregate message -> update/obtain new representations\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr, size=None)  # (N, H_out)\n",
    "        # out += x_r.mean(dim=1) # add the self features\n",
    "\n",
    "        alpha = self._alpha  # (#edges, 1)\n",
    "        assert alpha is not None, 'Alpha weights can not be None value!'\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        # Add in-degree and out-degree informations\n",
    "        in_degree = degree(edge_index[1]).to(torch.long)\n",
    "        out_degree = degree(edge_index[0]).to(torch.long)\n",
    "        out = (\n",
    "            out \n",
    "            + self.in_degree_encoder(in_degree) \n",
    "            + self.out_degree_encoder(out_degree)\n",
    "        )\n",
    "\n",
    "        # Returning attention weights with computed hidden features\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            return out, alpha.mean(dim=1, keepdims=True)\n",
    "        else:\n",
    "            return out  # (N, H_out)\n",
    "\n",
    "    def message(self, x_j, x_i, index, size_i, edge_attr):\n",
    "        # x_j has shape [#edges, NH, H_out]\n",
    "        # x_i has shape [#edges, NH, H_out]\n",
    "        # index: target node indexes, where data flows 'source_to_target': this is for computing softmax\n",
    "        # size: size_i, size_j mean num_nodes in the graph\n",
    "\n",
    "        x = x_i + x_j  # adding(element-wise) source and target node features together to calculate attention\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1)  # (#edges, NH)\n",
    "        alpha = softmax(alpha, index,\n",
    "                        num_nodes=size_i)  # spares softmax: groups node's attention and then node-wise softmax\n",
    "        self._alpha = alpha.mean(dim=1, keepdims=True)  # (#edges, 1)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # randomly dropping attention during training\n",
    "        node_out = (x_j * alpha.unsqueeze(-1)).mean(dim=1)\n",
    "\n",
    "        if self.inf is not None and edge_attr is not None:\n",
    "            if self.edge_dim != edge_attr.size(-1):\n",
    "                raise ValueError(\"Node and edge feature dimensionality do not \"\n",
    "                                 \"match. Consider setting the 'edge_dim' \"\"attribute\")\n",
    "            edge_attr = self.inf(self._alpha * edge_attr)  # transformed edge features via influence mechanism\n",
    "            return node_out + edge_attr  # (#edges, H_out)\n",
    "        return node_out  # (#edges, H_out)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        aggr_out += (1 + self.eps) * x[1].mean(dim=1)  # add the self features with a weighting factor\n",
    "        return aggr_out  # (N, H_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EINModel_v4(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dim_h, final_dim, num_heads, edge_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Layers\n",
    "        self.conv1 = EINv4(input_dim, dim_h, edge_dim=edge_dim,\n",
    "                           heads=num_heads, **kwargs)\n",
    "        self.conv2 = EINv4(dim_h, dim_h, edge_dim=edge_dim,\n",
    "                           heads=num_heads, **kwargs)\n",
    "        self.conv3 = EINv4(dim_h, dim_h, edge_dim=edge_dim,\n",
    "                           heads=num_heads, **kwargs)\n",
    "        self.conv4 = EINv4(dim_h, dim_h, edge_dim=edge_dim,\n",
    "                           heads=num_heads, **kwargs)\n",
    "        self.conv5 = EINv4(dim_h, dim_h, edge_dim=edge_dim,\n",
    "                           heads=num_heads, **kwargs)\n",
    "\n",
    "        # Linear layer\n",
    "        self.lin1 = Linear(dim_h * 5, dim_h * 5)\n",
    "\n",
    "        # Classification head\n",
    "        # final_dim: for classification or regression task\n",
    "        self.lin2 = Linear(dim_h * 5, final_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, type='binary'):\n",
    "        # Embedding\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h1 = h1.relu()\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h2 = h2.relu()\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "        h3 = h3.relu()\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr)\n",
    "        h4 = h4.relu()\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr)\n",
    "        h5 = h5.relu()\n",
    "\n",
    "        # Graph-level readout\n",
    "        h1 = global_mean_pool(h1, batch)\n",
    "        h2 = global_mean_pool(h2, batch)\n",
    "        h3 = global_mean_pool(h3, batch)\n",
    "        h4 = global_mean_pool(h4, batch)\n",
    "        h5 = global_mean_pool(h5, batch)\n",
    "\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h.flatten()\n",
    "        # return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EINModel_v4(input_dim=9, dim_h=64, final_dim=1, edge_dim=5, num_heads=16, eps=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    res = model.forward(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "    print(res.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
